model{

	# comparison acuity (alpha)
	groupALPHAmu        ~ dnorm(0,1/(100^2)) T(0,) ## UPDATED SINCE PAPER
	groupALPHAsigma     ~ dunif(0,500) ## UPDATED SINCE PAPER

	# error rates (epsilon)
	groupW          ~ dbeta(1.1, 10.9)  # mode for lapse rate
	groupKminus2    ~ dgamma(0.5,0.5) # concentration parameter ## UPDATED SINCE PAPER
	groupK          <- groupKminus2+2

	# priors
	for (p in participantIndexList){
		epsilon[p]  ~ dbeta(groupW*(groupK-2)+1 , (1-groupW)*(groupK-2)+1 ) T(,0.5)
		alpha[p]    ~ dnorm(groupALPHAmu, 1/(groupALPHAsigma^2)) T(0,)

		# PRIOR OVER DISCOUNT FRACTION
		for (d in 1:length(uniqueDelays)) {

			# 'safe' prior, centred on 1, with a large variance, but truncated
			discountFraction[p,d] ~ dnorm( 1, 1/ 1^2) T(0,5)

			# normal prior (centred on 1) but whose variance increases with delay
			#discountFraction[p,d] ~ dnorm( 1, 1/(0.000001 + (0.01 * uniqueDelays[d]) )) T(0,5)
		}
	}

	#  phi() cannot be vectorised

	for (t in 1:length(ID)) {
		VA[t] <- A[t] # NO DELAY FOR A
		VB[t] <- (B[t]) * discountFraction[ID[t], delayLookUp[t] ]

		# Psychometric function
		P[t] <- epsilon[ID[t]] + (1-2*epsilon[ID[t]]) * phi( (VB[t]-VA[t]) / alpha[ID[t]] )

		R[t]         ~ dbern(P[t]) # likelihood of actual response
		Rpostpred[t] ~ dbern(P[t]) # posterior predicted response
	}

}
